{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlled Accent Conversion - Inference Demo\n",
    "\n",
    "This notebook demonstrates inference with the Controlled Accent Conversion model.\n",
    "\n",
    "**Paper**: [Controlled Accent Conversion](https://arxiv.org/abs/2510.10785) (ICASSP 2026)\n",
    "\n",
    "**Demo**: [Listen to samples](https://claussss.github.io/accent_control_demo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Download checkpoints and stats from [Google Drive](https://drive.google.com/drive/folders/1Pnq_XV5VA_hcIpoOYfbSnLZFA3GKGk1C?usp=sharing)\n",
    "2. Place `stats/` folder and checkpoint in your project directory\n",
    "3. Install dependencies: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your paths below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CONFIGURE PATHS HERE ============\n",
    "\n",
    "# Path to the diffusion model checkpoint\n",
    "CHECKPOINT_PATH = './checkpoints/model_exp_lin_sched_100_step_snr_17.pt'\n",
    "\n",
    "# Path to stats directory (contains mean/std .pt files)\n",
    "STATS_PATH = './stats'\n",
    "\n",
    "# Path to audio file for conversion\n",
    "AUDIO_PATH = './sample_audio.wav'\n",
    "\n",
    "# Transcript of the audio (required for phoneme alignment)\n",
    "TRANSCRIPT = \"To my dearest and always appreciated friend I submit myself\"\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda'  # or 'cpu'\n",
    "\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "import re\n",
    "\n",
    "# Phonemizer imports\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "# ASR model for forced alignment\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    ")\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "\n",
    "# FACodec\n",
    "sys.path.append('../Amphion')\n",
    "from Amphion.models.codec.ns3_codec import FACodecEncoder, FACodecDecoder\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Our modules\n",
    "sys.path.append('.')\n",
    "from FACodec_AC.models import DenoisingTransformerModel\n",
    "from FACodec_AC.config import Config\n",
    "from FACodec_AC.utils import (\n",
    "    get_phone_forced_alignment, \n",
    "    interpolate_alignment, \n",
    "    QuantizerNames, \n",
    "    get_z_from_indx, \n",
    "    snap_latent,\n",
    "    standardize,\n",
    "    destandardize\n",
    ")\n",
    "from IPython.display import Audio\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize FACodec Encoder/Decoder\n",
    "\n",
    "FACodec is used to encode audio into discrete representations and decode them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FACodec\n",
    "fa_encoder = FACodecEncoder(\n",
    "    ngf=32,\n",
    "    up_ratios=[2, 4, 5, 5],\n",
    "    out_channels=256\n",
    ")\n",
    "fa_decoder = FACodecDecoder(\n",
    "    in_channels=256,\n",
    "    upsample_initial_channel=1024,\n",
    "    ngf=32,\n",
    "    up_ratios=[5, 5, 4, 2],\n",
    "    vq_num_q_c=2,\n",
    "    vq_num_q_p=1,\n",
    "    vq_num_q_r=3,\n",
    "    vq_dim=256,\n",
    "    codebook_dim=8,\n",
    "    codebook_size_prosody=10,\n",
    "    codebook_size_content=10,\n",
    "    codebook_size_residual=10,\n",
    "    use_gr_x_timbre=True,\n",
    "    use_gr_residual_f0=True,\n",
    "    use_gr_residual_phone=True,\n",
    ")\n",
    "\n",
    "# Load pretrained weights\n",
    "encoder_ckpt = hf_hub_download(repo_id=\"amphion/naturalspeech3_facodec\", filename=\"ns3_facodec_encoder.bin\")\n",
    "decoder_ckpt = hf_hub_download(repo_id=\"amphion/naturalspeech3_facodec\", filename=\"ns3_facodec_decoder.bin\")\n",
    "fa_encoder.load_state_dict(torch.load(encoder_ckpt))\n",
    "fa_decoder.load_state_dict(torch.load(decoder_ckpt))\n",
    "fa_encoder.eval()\n",
    "fa_decoder.eval()\n",
    "fa_encoder = fa_encoder.to(DEVICE)\n",
    "fa_decoder = fa_decoder.to(DEVICE)\n",
    "\n",
    "print(\"FACodec loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Phoneme Alignment Pipeline\n",
    "\n",
    "We use Wav2Vec2 for forced phoneme alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ASR model for forced alignment\n",
    "fe = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")\n",
    "tok = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\", use_fast=False)\n",
    "proc = Wav2Vec2Processor(feature_extractor=fe, tokenizer=tok)\n",
    "asr_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\").to(DEVICE)\n",
    "asr_model.eval()\n",
    "target_sr = fe.sampling_rate  # 16000\n",
    "\n",
    "# Phonemizer setup\n",
    "REMOVE = ''.join(['ʲ', 'ʷ', 'ʰ', 'ʱ', 'ˠ', 'ˤ', 'ʶ', 'ʵ'])\n",
    "pipeline = {\n",
    "    \"normaliser\": Normalizer(lang=\"en\", input_case=\"cased\", deterministic=True, post_process=True),\n",
    "    \"regex\": re.compile(r\"[^a-z' ]\"),\n",
    "    \"sep\": Separator(phone=\" \", word=\"|\", syllable=\"\"),\n",
    "    \"backend\": EspeakBackend('en-us'),\n",
    "    'wav2vec_processor': proc,\n",
    "    'wav2vec_model': asr_model,\n",
    "    'DROP_RE': re.compile('[%s]' % re.escape(REMOVE))\n",
    "}\n",
    "\n",
    "print(\"Phoneme alignment pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Diffusion Model and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the diffusion model\n",
    "diffusion_model = DenoisingTransformerModel(\n",
    "    d_model=Config.d_model,\n",
    "    nhead=Config.nhead,\n",
    "    num_layers=Config.num_layers,\n",
    "    d_ff=Config.d_ff,\n",
    "    dropout=Config.dropout,\n",
    "    max_seq_len=Config.max_seq_len,\n",
    "    FACodec_dim=Config.FACodec_dim,\n",
    "    phone_vocab_size=Config.PHONE_VOCAB_SIZE,\n",
    "    num_steps=100,  # Number of diffusion steps\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "diffusion_model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
    "diffusion_model = diffusion_model.to(DEVICE)\n",
    "diffusion_model.eval()\n",
    "\n",
    "# Load normalization stats\n",
    "zc1_mean = torch.load(os.path.join(STATS_PATH, 'mean_zc1_indx.pt')).to(DEVICE)\n",
    "zc1_std = torch.load(os.path.join(STATS_PATH, 'std_zc1_indx.pt')).to(DEVICE)\n",
    "zc2_mean = torch.load(os.path.join(STATS_PATH, 'mean_zc2_indx.pt')).to(DEVICE)\n",
    "zc2_std = torch.load(os.path.join(STATS_PATH, 'std_zc2_indx.pt')).to(DEVICE)\n",
    "\n",
    "print(f\"Diffusion model loaded from: {CHECKPOINT_PATH}\")\n",
    "print(f\"Stats loaded from: {STATS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Process Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "wav_waveform, wav_sr = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "# Convert to mono if stereo\n",
    "if wav_waveform.shape[0] > 1:\n",
    "    wav_waveform = wav_waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "# Resample to 16kHz if needed\n",
    "if wav_sr != 16000:\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=wav_sr, new_freq=16000)\n",
    "    wav_waveform = resample(wav_waveform)\n",
    "\n",
    "wav_waveform = wav_waveform.to(DEVICE)\n",
    "print(f\"Audio shape: {wav_waveform.shape}\")\n",
    "\n",
    "# Play original audio\n",
    "Audio(wav_waveform.cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract FACodec Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode with FACodec\n",
    "with torch.no_grad():\n",
    "    h_input = fa_encoder(wav_waveform[None, :, :])\n",
    "    vq_post_emb, vq_id, _, quantized_arr, spk_embs = fa_decoder(h_input, eval_vq=False, vq=True)\n",
    "\n",
    "# Content indices\n",
    "zc1_indx = vq_id[1]\n",
    "seq_len = zc1_indx.shape[1]\n",
    "print(f\"Sequence length: {seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get Phoneme Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transcript\n",
    "file_id = os.path.splitext(os.path.basename(AUDIO_PATH))[0]\n",
    "transcript_dict = {file_id: TRANSCRIPT}\n",
    "audio_folder = os.path.dirname(AUDIO_PATH)\n",
    "\n",
    "# Get forced alignment\n",
    "predicted_ids, _, frames_score, _ = get_phone_forced_alignment(\n",
    "    embedding_path=file_id + '.pt',\n",
    "    audio_folder=audio_folder,\n",
    "    transcript_metadata=transcript_dict,\n",
    "    device=DEVICE,\n",
    "    target_sr=target_sr,\n",
    "    pipeline=pipeline,\n",
    "    inference=True\n",
    ")\n",
    "\n",
    "# Interpolate to match FACodec sequence length\n",
    "interpolated_phone_ids = interpolate_alignment(predicted_ids, seq_len)[0]\n",
    "print(f\"Phone IDs shape: {interpolated_phone_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DDIM Sampling\n",
    "\n",
    "We use DDIM (Denoising Diffusion Implicit Models) for faster sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ddim_schedule(num_train_steps: int, num_infer_steps: int) -> torch.LongTensor:\n",
    "    \"\"\"Create DDIM schedule for inference.\"\"\"\n",
    "    t = np.linspace(num_train_steps - 1, 0, num_infer_steps)\n",
    "    return torch.tensor(np.round(t), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "\n",
    "def sample_ddim(model, zc1_clean, padded_phone_ids, padding_mask, schedule, start_idx=0, seed=42):\n",
    "    \"\"\"DDIM sampling for accent conversion.\"\"\"\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    model.eval()\n",
    "    bsz, C, L = zc1_clean.shape\n",
    "\n",
    "    sqrt_abar = model.sqrt_abar\n",
    "    sqrt_1mabar = model.sqrt_1mabar\n",
    "\n",
    "    # Forward diffuse to starting step\n",
    "    t_start = schedule[start_idx]\n",
    "    eps0 = torch.randn_like(zc1_clean)\n",
    "    z_t = sqrt_abar[t_start].view(1, 1, 1) * zc1_clean + sqrt_1mabar[t_start].view(1, 1, 1) * eps0\n",
    "\n",
    "    # Reverse DDIM sampling\n",
    "    with torch.no_grad():\n",
    "        for i in range(start_idx, len(schedule)):\n",
    "            t = schedule[i]\n",
    "            t_idx = torch.full((bsz,), t, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "            eps_pred, _ = model(\n",
    "                zc1_noisy=z_t,\n",
    "                padded_phone_ids=padded_phone_ids,\n",
    "                t=t_idx,\n",
    "                padding_mask=padding_mask\n",
    "            )\n",
    "\n",
    "            sa = sqrt_abar[t].view(1, 1, 1)\n",
    "            s1a = sqrt_1mabar[t].view(1, 1, 1)\n",
    "            x0_hat = (z_t - s1a * eps_pred) / sa\n",
    "\n",
    "            if i < len(schedule) - 1:\n",
    "                t_prev = schedule[i + 1]\n",
    "                z_t = sqrt_abar[t_prev].view(1, 1, 1) * x0_hat + sqrt_1mabar[t_prev].view(1, 1, 1) * eps_pred\n",
    "            else:\n",
    "                z_t = x0_hat\n",
    "\n",
    "    zc1_pred = z_t\n",
    "\n",
    "    # Predict zc2 from clean zc1\n",
    "    t_zero = torch.zeros((bsz,), device=DEVICE, dtype=torch.long)\n",
    "    _, zc2_pred = model(\n",
    "        zc1_noisy=zc1_pred,\n",
    "        padded_phone_ids=padded_phone_ids,\n",
    "        t=t_zero,\n",
    "        padding_mask=padding_mask\n",
    "    )\n",
    "\n",
    "    return zc1_pred, zc2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Accent Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input\n",
    "x_input = get_z_from_indx(\n",
    "    vq_id[1], fa_decoder=fa_decoder,\n",
    "    layer=0, quantizer_num=QuantizerNames.content, dim=Config.FACodec_dim\n",
    ")\n",
    "zc1_input_normalized = standardize(x_input, zc1_mean, zc1_std)\n",
    "\n",
    "# Create DDIM schedule\n",
    "T = 100  # Training steps\n",
    "K = 100  # Inference steps\n",
    "schedule = make_ddim_schedule(T, K)\n",
    "\n",
    "# Sampling parameters\n",
    "noise_level = 0  # 0 = full diffusion from noise, higher = less noise\n",
    "seed = 42\n",
    "\n",
    "# Prepare tensors\n",
    "bsz, C, seq_len = zc1_input_normalized.shape\n",
    "padded_phone_ids = interpolated_phone_ids\n",
    "padding_mask = torch.zeros(bsz, seq_len, dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "# Run DDIM sampling\n",
    "zc1_pred, zc2_pred = sample_ddim(\n",
    "    diffusion_model,\n",
    "    zc1_input_normalized,\n",
    "    padded_phone_ids,\n",
    "    padding_mask,\n",
    "    schedule,\n",
    "    start_idx=noise_level,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Denormalize\n",
    "zc1_pred = destandardize(zc1_pred, zc1_mean, zc1_std)\n",
    "zc2_pred = destandardize(zc2_pred, zc2_mean, zc2_std)\n",
    "\n",
    "print(\"Diffusion sampling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reconstruct Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap to codebook and project to 256-dim\n",
    "zc1_pred_snapped = snap_latent(zc1_pred.transpose(1, 2), fa_decoder, layer=0)\n",
    "zc1_pred_256 = fa_decoder.quantizer[1].layers[0].out_proj(zc1_pred_snapped).transpose(1, 2)\n",
    "zc2_pred_256 = fa_decoder.quantizer[1].layers[1].out_proj(zc2_pred.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "# Combine content codes\n",
    "x_reconstructed = zc1_pred_256 + zc2_pred_256\n",
    "\n",
    "# Decode with FACodec\n",
    "with torch.no_grad():\n",
    "    # Combine: prosody + predicted content + acoustic residuals\n",
    "    final_code = quantized_arr[0] + x_reconstructed + quantized_arr[2]\n",
    "    wav_output = fa_decoder.inference(final_code, spk_embs)\n",
    "\n",
    "print(\"Audio reconstruction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Listen to Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Audio:\")\n",
    "Audio(wav_waveform.cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converted Audio:\")\n",
    "Audio(wav_output.squeeze().cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Output (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save converted audio\n",
    "output_path = AUDIO_PATH.replace('.wav', '_converted.wav')\n",
    "torchaudio.save(output_path, wav_output.squeeze(0).cpu(), 16000)\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
